{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db7b79fb",
   "metadata": {},
   "source": [
    "## Why naïve?\n",
    "A naïve Bayes classifier is a probabilistic machine learning model that’s used for classification task. The crux of the classifier is based on the Bayes theorem.\n",
    "\n",
    "The classifier is naïve. Because of its assumptions \n",
    "1. all variables in the dataset are “naïve” i.e not correlated to each other\n",
    "2. all the predictors have an equal effect on the outcome\n",
    "\n",
    "## Types of Naive Bayes Classifier:\n",
    "\n",
    "1. Multinomial Naive Bayes: Used for multi category classification problem\n",
    "2. Bernoulli Naive Bayes: Similar to the multinomial naive bayes except that the predictors are boolean variables.\n",
    "3. Gaussian Naive Bayes: This is used Predictors are continuous valued. We assume that the predictor values are sampled from a gaussian distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe11903",
   "metadata": {},
   "source": [
    "## Bayesian Inference\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc2c7a3",
   "metadata": {},
   "source": [
    "## Advantages\n",
    "\n",
    "1. Needs less training data.\n",
    "2. This example shows binary outcome. However the algorithm also performs well in multi-class prediction (TODO).\n",
    "2. A Naive Bayes classifier performs better compared to other models like logistic regression and less training data is sufficient. (Variables should be independent)\n",
    "3. It performs well with categorical input variables compared to numerical variables. \n",
    "\n",
    "For numerical variable, normal distribution is assumed (This example uses numerical predictor variables)\n",
    "\n",
    "## Disadvantages\n",
    "1. Zero Frequency problem: If categorical variable has a category in test data set that was not observed in training data set, then model will assign a zero probability and will not make prediction. We use one of many smoothing technique to address this. One of the simplest smoothing techniques is called Laplace estimation. (https://www.quora.com/How-does-Laplacian-add-1-smoothing-work-for-a-Naive-Bayes-classfier-algorithm) TODO. A notebook on this later\n",
    "2. It is almost impossible to have completely independent predictors in real life and this classifier will not perform well in such cases.\n",
    "\n",
    "This notebook is meant to demonstrate the technique nonetheless.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "Dataset used is from here -  https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\n",
    "\n",
    "Dataset consists of medical predictor variables and one target variable Outcome. \n",
    "Predictor variables \n",
    "1. Pregnancies: Number of times pregnant\n",
    "2. Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "3. BloodPressure: Diastolic blood pressure (mm Hg)\n",
    "4. SkinThickness: Triceps skin fold thickness (mm)\n",
    "4. Insulin: 2-Hour serum insulin (mu U/ml)\n",
    "5. BMI: Body mass index (weight in kg/(height in m)^2)\n",
    "6. DiabetesPedigreeFunction: Diabetes pedigree function\n",
    "7. Age: Age (years)\n",
    "\n",
    "Outcome: Class variable (0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe9dad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "%matplotlib  inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edffde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "column = [\"Pregnancies\",\"Glucose\",\"BloodPressure\",\"SkinThickness\",\"Insulin\",\n",
    "          \"BMI\",\"DiabetesPedigreeFunction\",\"Age\",\"Outcome\"]\n",
    "\n",
    "#data = pd.read_csv('pima-indians-diabetes.data.csv',names=column)\n",
    "data = pd.read_csv(\"https://raw.githubusercontent.com/datavector-io/datascience/main/Bayesian/houseprices.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d20840",
   "metadata": {},
   "source": [
    "From Bayes theorem\n",
    "\n",
    "\\begin{equation}\n",
    "P(y \\mid x) =  \\dfrac{P(x \\mid y)\\,P(y)}{P(x)}\n",
    "\\end{equation}\n",
    "\n",
    "where,\n",
    "\n",
    "1. P(y|x) is the posterior probability of class y given predictor ( features).\n",
    "2. P(y) is the probability of class.\n",
    "3. P(x|y) is the likelihood which is the probability of predictor given class.\n",
    "4. P(x) is the prior probability of predictor.\n",
    "\n",
    "Or more preceisely\n",
    "\n",
    "\\begin{equation}\n",
    "P(y \\mid x_{1}, x_{2}, ... , x_{n} ) =  \\dfrac{P(x_{1}, x_{2}, ... , x_{n} \\mid y)\\,P(y)}{P(x_{1}, x_{2}, ... , x_{n})}\n",
    "\\end{equation}\n",
    "\n",
    "can be written as \n",
    "\n",
    "\\begin{equation}\n",
    "P(y \\mid x_{1}, x_{2}, ... , x_{n} ) =  \\dfrac{P(x_{1} \\mid y) \\, P(x_{2} \\mid y), ... \\, \\, P(x_{n} \\mid y) \\, P(y)}{P(x_{1}) \\, P(x_{2})\\, ... \\, P(x_{n})}\n",
    "\\end{equation}\n",
    "\n",
    "For all entries in the dataset, the denominator does not change, it remain static. Therefore, the denominator can be removed and a proportionality can be introduced\n",
    "\n",
    "\\begin{equation}\n",
    "P(y \\mid x_{1}, x_{2}, ... , x_{n} ) \\, \\propto  \\, P(x_{1} \\mid y) \\, P(x_{2} \\mid y) \\, ... \\, P(x_{n} \\mid y) \\, P(y)\n",
    "\\end{equation}\n",
    "\n",
    "Rewriting in short form\n",
    "\n",
    "\\begin{equation}\n",
    "P(y \\mid x_{1}, x_{2}, ... , x_{n} ) \\, \\propto P(y) \\, \\prod_{i=1}^n \\,P(x_{i} \\mid y)\n",
    "\\end{equation}\n",
    "\n",
    "In our case, the class variable(y) has only two outcomes, 1 or 0. There could be cases where the classification could be multivariate. Therefore, we need to find the class y with maximum probability such that:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "y = argmax_{y} \\, \\prod_{i=1}^n \\,P(x_{i} \\mid y)\n",
    "\\end{aligned}\n",
    "\\tag{Equation 1}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\end{equation}\n",
    "\n",
    "To find the maxima, we need to calculate derivative of the RHS in above equation and equate to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11940c42",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4458808",
   "metadata": {},
   "source": [
    "Assuming a Gausian distribution for each of the numerical predictor variables in the dataset, each $ P(x_{i} \\mid y) $ takes the form \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "P(x_{i} \\mid y) = \\dfrac{1}{\\sigma_{y}\\sqrt{2\\pi}} \\, e^{ {\\dfrac{-1}{2} \\, (\\dfrac{x_{i} \\, - \\, \\mu_{y}}{ \\sigma })}^{2}}\n",
    "\\end{aligned}\n",
    "\\tag{Equation 1}\n",
    "\\end{equation}\n",
    "\n",
    "Calculating derivative of a long product like above is tedious. Instead we take the log on both sides and take its derivative. We can use several properties of log to simplify this. The main reason we are able to take derivative of either the function or the log is because the function is monotonous and both the function and its derivative peak at the same point.\n",
    "\n",
    "At that point were we get a argmax, the x is such such that it is the mean of the of all the $ x_{i} $ with corresponding $ \\sigma $ This overlaps with the maximum likelihood estimation (MLE) techniques.\n",
    "\n",
    "Details of derivation and some intuition into the MLE can be found in this stat quest: \n",
    "1. https://www.youtube.com/watch?v=XepXtl9YKwc\n",
    "2. https://www.youtube.com/watch?v=Dn6b9fCIUpM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c6d524",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d84d431",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "065dafce",
   "metadata": {},
   "source": [
    "# Acknowledgements\n",
    "1. https://github.com/2796gaurav/Naive-bayes-explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5977bce9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
