{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db7b79fb",
   "metadata": {},
   "source": [
    "## Why naïve?\n",
    "A naïve Bayes classifier is a probabilistic machine learning model that’s used for classification task. The crux of the classifier is based on the Bayes theorem.\n",
    "\n",
    "The classifier is naïve. Because of its assumptions \n",
    "1. all variables in the dataset are “naïve” i.e not correlated to each other\n",
    "2. all the predictors have an equal effect on the outcome\n",
    "\n",
    "## Types of Naive Bayes Classifier:\n",
    "\n",
    "1. Multinomial Naive Bayes: Used for multi category classification problem\n",
    "2. Bernoulli Naive Bayes: Similar to the multinomial naive bayes except that the predictors are boolean variables.\n",
    "3. Gaussian Naive Bayes: This is used Predictors are continuous valued. We assume that the predictor values are sampled from a gaussian distribution.\n",
    "\n",
    "## Advantages\n",
    "\n",
    "1. Needs less training data.\n",
    "2. This example shows binary outcome. However the algorithm also performs well in multi-class prediction (TODO).\n",
    "2. A Naive Bayes classifier performs better compared to other models like logistic regression and less training data is sufficient. (Variables should be independent)\n",
    "3. It performs well with categorical input variables compared to numerical variables. \n",
    "\n",
    "For numerical variable, normal distribution is assumed (This example uses numerical predictor variables)\n",
    "\n",
    "## Disadvantages\n",
    "1. Zero Frequency problem: If categorical variable has a category in test data set that was not observed in training data set, then model will assign a zero probability and will not make prediction. We use one of many smoothing technique to address this. One of the simplest smoothing techniques is called Laplace estimation. (https://www.quora.com/How-does-Laplacian-add-1-smoothing-work-for-a-Naive-Bayes-classfier-algorithm) TODO. A notebook on this later\n",
    "2. It is almost impossible to have completely independent predictors in real life and this classifier will not perform well in such cases.\n",
    "\n",
    "This notebook is meant to demonstrate the technique nonetheless"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe11903",
   "metadata": {},
   "source": [
    "## Bayesian Inference\n",
    "\n",
    "1. Statistical inference is the process of deducing properties about a population (hence about its probability distribution) from data. Using standard Maximum Likelihood Estimation technique, we can determine the maximum likelihood esitmate of a mean from a set of observed data points. \n",
    "2. Bayesian inference is therefore just the process of deducing properties about a population or probability distribution from data using Bayes’ theorem\n",
    "\n",
    "NOTE: Probability and Likelihood are different beasts in technical terms. https://www.youtube.com/watch?v=pYxNSUDSFH4\n",
    "\n",
    "## Using Bayes’ theorem with distributions\n",
    "\n",
    "### Standard Bayes' theorem reproduced below:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "P(A \\mid B) =  \\dfrac{P(B \\mid A)\\,P(A)}{P(B)}\n",
    "\\end{aligned}\n",
    "\\tag{Equation 1}\n",
    "\\end{equation}\n",
    "\n",
    "1. P(A|B) is called the posterior; this is what we are trying to estimate. In the above example, this would be the “probability of having cancer given that the person is a smoker”.\n",
    "2. P(A) is called the prior; this is the probability of our hypothesis without any additional prior information. In the above example, this would be the “probability of having cancer”.\n",
    "3. P(B|A) is called the likelihood; this is the probability of observing the new evidence, given our initial hypothesis. In the above example, this would be the “probability of being a smoker given that the person has cancer”.\n",
    "4. P(B) is called the marginal likelihood; this is the total probability of observing the evidence. In the above example, this would be the “probability of being a smoker”. In many applications of Bayes Rule, this is ignored, as it mainly serves as normalization.\n",
    "\n",
    "### Modifications\n",
    "\n",
    "Bayes' theorem can be used beyond numbers with two particular changes. We will replace to get a new form of the equation\n",
    "\n",
    "1. B with data\n",
    "2. A with $ \\theta $, (predictors)\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "P(\\theta \\mid data) =  \\dfrac{P(data \\mid \\theta)\\,P(\\theta)}{P(data)}\n",
    "\\end{aligned}\n",
    "\\tag{Equation 2}\n",
    "\\end{equation}\n",
    "\n",
    "Here $\\theta $ can represent a single predictor variable or vector of predictor variables.  Following techniques will be adopted for each of RHS\n",
    "\n",
    "1. By using a distribution for $ P(\\theta) $\n",
    "2. By using maximum likelihood techniques for $ P(data \\mid \\theta) $\n",
    "3. Ignoring P(data) (Very hard to calculate and it can be ignored as explained below)\n",
    "\n",
    "We now have new names for each of the Bayes' theorem participants:\n",
    "1. $ P(\\theta \\mid data) $ is called the posterior distribution\n",
    "2. $ P(\\theta) $ is called the prior distribution. Generally this would be the distribution of each of the predictor variable for the entire training set\n",
    "3. $ P(data \\mid \\theta) $ is called the likelihood distribution. Sometimes it is also written as $\\mathcal{L}(data \\mid \\theta) $\n",
    "\n",
    "### Expanding on Bayes' theorem adaptation to distribution\n",
    "\n",
    "Here I am using x(s) for Theta.\n",
    "\n",
    "From Bayes theorem\n",
    "\n",
    "\\begin{equation}\n",
    "P(y \\mid x) =  \\dfrac{P(x \\mid y)\\,P(y)}{P(x)}\n",
    "\\end{equation}\n",
    "\n",
    "where,\n",
    "\n",
    "1. P(y|x) is the posterior probability of class y given predictor (aka features).\n",
    "2. P(y) is the probability of class.\n",
    "3. P(x|y) is the likelihood which is the probability of predictor given class.\n",
    "4. P(x) is the prior probability of predictor.\n",
    "\n",
    "Or more preceisely\n",
    "\n",
    "\\begin{equation}\n",
    "P(y \\mid x_{1}, x_{2}, ... , x_{n} ) =  \\dfrac{P(x_{1}, x_{2}, ... , x_{n} \\mid y)\\,P(y)}{P(x_{1}, x_{2}, ... , x_{n})}\n",
    "\\end{equation}\n",
    "\n",
    "can be written as \n",
    "\n",
    "\\begin{equation}\n",
    "P(y \\mid x_{1}, x_{2}, ... , x_{n} ) =  \\dfrac{P(x_{1} \\mid y) \\, P(x_{2} \\mid y), ... \\, \\, P(x_{n} \\mid y) \\, P(y)}{P(x_{1}) \\, P(x_{2})\\, ... \\, P(x_{n})}\n",
    "\\end{equation}\n",
    "\n",
    "For all entries in the dataset, the denominator does not change, it remain static. Therefore, the denominator can be removed and a proportionality can be introduced\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "P(y \\mid x_{1}, x_{2}, ... , x_{n} ) \\, \\propto  \\, P(x_{1} \\mid y) \\, P(x_{2} \\mid y) \\, ... \\, P(x_{n} \\mid y) \\, P(y)\n",
    "\\end{aligned}\n",
    "\\tag{Equation 3}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Rewriting in short form\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "P(y \\mid x_{1}, x_{2}, ... , x_{n} ) \\, \\propto P(y) \\, \\prod_{i=1}^n \\,P(x_{i} \\mid y)\n",
    "\\end{aligned}\n",
    "\\tag{Equation 4}\n",
    "\\end{equation}\n",
    "\n",
    "The two terms in equation 3 are analyzed separately\n",
    "1. P(y) by looking at its distribution. \n",
    "2. $ \\prod_{i=1}^n \\,P(x_{i} \\mid y) $ is analyzed with Maximum Likelihood Techniques\n",
    "\n",
    "But first let us look at the dataset to which we will apply these"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc2c7a3",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Dataset used is from here -  https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\n",
    "\n",
    "Dataset consists of medical predictor variables and one target variable Outcome. \n",
    "Predictor variables \n",
    "1. Pregnancies: Number of times pregnant\n",
    "2. Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "3. BloodPressure: Diastolic blood pressure (mm Hg)\n",
    "4. SkinThickness: Triceps skin fold thickness (mm)\n",
    "4. Insulin: 2-Hour serum insulin (mu U/ml)\n",
    "5. BMI: Body mass index (weight in kg/(height in m)^2)\n",
    "6. DiabetesPedigreeFunction: Diabetes pedigree function\n",
    "7. Age: Age (years)\n",
    "\n",
    "Outcome: Class variable (0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe9dad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "%matplotlib  inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edffde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "column = [\"Pregnancies\",\"Glucose\",\"BloodPressure\",\"SkinThickness\",\"Insulin\",\n",
    "          \"BMI\",\"DiabetesPedigreeFunction\",\"Age\",\"Outcome\"]\n",
    "\n",
    "#data = pd.read_csv('pima-indians-diabetes.data.csv',names=column)\n",
    "git_file_path = \"https://raw.githubusercontent.com/datavector-io/datascience/main/Bayesian/pima-indians-diabetes.data.csv\"\n",
    "data = pd.read_csv(git_file_path, names=column)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce90c209",
   "metadata": {},
   "source": [
    "We apply the equation 3 now. \n",
    "\n",
    "The predictor variables \"Pregnancies\",\"Glucose\",\"BloodPressure\" can be thought of as $ x_{1} x_{2} ... x_{n} $ etc.\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "P(y \\mid x_{1}, x_{2}, ... , x_{n} ) \\, \\propto  \\, P(x_{1} \\mid y) \\, P(x_{2} \\mid y) \\, ... \\, P(x_{n} \\mid y)\\, P(y) \n",
    "\\end{equation}\n",
    "\n",
    "i.e. $ P(Outcome=0 \\mid Data ) \\, $ is proportional to the following product of probabilities:\n",
    "\n",
    "\\begin{equation}\n",
    "P(Outcome=0) \\,\\, P(Pregnancies \\mid Outcome=0) \\, P(Glucose \\mid Outcome=0) \\, P(BloodPressure \\mid Outcome=0) \\, P(SkinThickness \\mid Outcome=0) \\, P(Insulin \\mid Outcome=0) \\, P(BMI \\mid Outcome=0) \\, P(DiabetesPedigreeFunction \\mid Outcome=0) \\, P(Age \\mid Outcome=0) \n",
    "\\end{equation}\n",
    "\n",
    "1. P(Outcome=0)is simply the total probability that outcome=0 in the existing dataset\n",
    "2. P(Pregnancies | Outcome=0) \\, P(Glucose | Outcome=0) is the likelihood of each predictor variable distribution\n",
    "\n",
    "<b>NOTE: Each term in the likelihood is really not independent. But that is the naive assumption we do in this algorithm</b>\n",
    "\n",
    "But first we need to split train and test set. Then we can do the likelihood calc for each term"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b31fc2b",
   "metadata": {},
   "source": [
    "## Shuffle and Split data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0c5465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle dataset with sample\n",
    "data = data.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "print(data.shape)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e65103c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70-30 split\n",
    "total_record_count = len(data.index)\n",
    "train_record_count = round(0.7 * total_record_count)\n",
    "test_record_count = total_record_count - train_record_count\n",
    "\n",
    "train_records = data.iloc[:train_record_count,:]\n",
    "test_records = data.iloc[train_record_count:,:]\n",
    "print(\"Shape of new dataframes - {} , {}\".format(train_records.shape, test_records.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12882b5d",
   "metadata": {},
   "source": [
    "## Step 1: Calculate Prior Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d15a7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_outcome_0_num = train_records['Outcome'][train_records['Outcome'] == 0].count()\n",
    "train_outcome_1_num = train_records['Outcome'][train_records['Outcome'] == 1].count()\n",
    "\n",
    "# Total people\n",
    "train_total_num = train_records['Outcome'].count()\n",
    "print(\"Train Total = {} , Outcome 0 Sum = {}, Outcome 1 Sum = {}\".format(train_total_num, train_outcome_0_num, train_outcome_1_num))\n",
    "\n",
    "train_p_outcome_0 = train_outcome_0_num/train_total_num\n",
    "train_p_outcome_1 = train_outcome_1_num/train_total_num\n",
    "print(\"P(Outcome=0) = {} , P(Outcome=1) = {}\".format(train_p_outcome_0, train_p_outcome_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20122f24",
   "metadata": {},
   "source": [
    "## Step 1: Calculate Likelihood\n",
    "\n",
    "Since we assume gaussian distribution for all predictor variables, it can be proved that the max likelihood occurs for mean. (Links below in Maximum Likelihood Estimation).\n",
    "\n",
    "Hence we now need to calculate the mean and std dev so as to substitute in the Guassian distribution function and get the likelihood value\n",
    "\n",
    "\\begin{aligned}\n",
    "P(x_{i} \\mid y) = \\dfrac{1}{\\sigma_{y}\\sqrt{2\\pi}} \\, e^{ {\\dfrac{-1}{2} \\, (\\dfrac{x_{i} \\, - \\, \\mu_{y}}{ \\sigma })}^{2}}\n",
    "\\end{aligned}\n",
    "\n",
    "And for that purpose we need mean and standard deviation for each of the predictor variables and also further sub divided by Outcome=0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777bc50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the means of each predictor divided to outcome\n",
    "\n",
    "data_means = data.groupby('Outcome').mean()\n",
    "data_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04aa4e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the std dev of each predictor divided to outcome\n",
    "data_stddev = data.groupby('Outcome').std()\n",
    "data_stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94a04ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_likelihood(x, mean_y, stddev_y):\n",
    "    p = 1/(stddev_y*np.sqrt(2*np.pi)) * np.exp((-(x-mean_y)**2)/(2*(stddev_y**2)))\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b23663",
   "metadata": {},
   "source": [
    "We need these probabilities for outcome=0 to calculate posterior probabilities for outcome = 0\n",
    "\\begin{equation}\n",
    "P(Pregnancies \\mid Outcome=0) \\, P(Glucose \\mid Outcome=0) \\, P(BloodPressure \\mid Outcome=0) \\, P(SkinThickness \\mid Outcome=0) \\, P(Insulin \\mid Outcome=0) \\, P(BMI \\mid Outcome=0) \\, P(DiabetesPedigreeFunction \\mid Outcome=0) \\, P(Age \\mid Outcome=0) \n",
    "\\end{equation}\n",
    "\n",
    "and we need these probabilities for outcome=1 to calculate posterior probabilities for outcome = 1\n",
    "\\begin{equation}\n",
    "P(Pregnancies \\mid Outcome=1) \\, P(Glucose \\mid Outcome=1) \\, P(BloodPressure \\mid Outcome=1) \\, P(SkinThickness \\mid Outcome=1) \\, P(Insulin \\mid Outcome=1) \\, P(BMI \\mid Outcome=1) \\, P(DiabetesPedigreeFunction \\mid Outcome=1) \\, P(Age \\mid Outcome=1) \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bb4b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterate over test records and \n",
    "# 1. fetch mean and std dev for outcome = 0 for each predictors\n",
    "# 2. use them to calculate likelihood(predictor | outcome=0) by passing the mean & std dev of each predictor along with the test predictor\n",
    "# 3. Repeat 2 for outcome = 1 predictor variables \n",
    "# 4. compare probability and whichever is greater is the predicted outcome\n",
    "\n",
    "#likelihood_pregnancy_outcome0 = gaussian_likelihood() ....."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d20840",
   "metadata": {},
   "source": [
    "## Maximum Likelihood estimation\n",
    "In our case, the class variable(y) has only two outcomes, 1 or 0. There could be cases where the classification could be multivariate. Therefore, we need to find the class y with maximum probability such that the probability product in equation 3 is maximized:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "y = argmax_{y} \\, \\prod_{i=1}^n \\,P(x_{i} \\mid y)\n",
    "\\end{aligned}\n",
    "\\tag{Equation 4}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\end{equation}\n",
    "\n",
    "To find the maxima, we need to calculate derivative of the RHS in above equation and equate to 0.\n",
    "\n",
    "Assuming a Gausian distribution for each of the numerical predictor variables in the dataset, each $ P(x_{i} \\mid y) $ takes the form \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "P(x_{i} \\mid y) = \\dfrac{1}{\\sigma_{y}\\sqrt{2\\pi}} \\, e^{ {\\dfrac{-1}{2} \\, (\\dfrac{x_{i} \\, - \\, \\mu_{y}}{ \\sigma })}^{2}}\n",
    "\\end{aligned}\n",
    "\\tag{Equation 5}\n",
    "\\end{equation}\n",
    "\n",
    "Calculating derivative of a long product like above is tedious. Instead we take the log on both sides and take its derivative. We can use several properties of log to simplify this. The main reason we are able to take derivative of either the function or the log is because the function is monotonous and both the function and its derivative peak at the same point.\n",
    "\n",
    "At that point were we get a argmax, the x is such such that it is the mean of the of all the $ x_{i} $ with corresponding $ \\sigma $ \n",
    "\n",
    "A intuitive understanding of maximum likelihood estimation (MLE) techniques and details of derivation can be found in this stat quest: \n",
    "1. https://www.youtube.com/watch?v=XepXtl9YKwc\n",
    "2. https://www.youtube.com/watch?v=Dn6b9fCIUpM\n",
    "\n",
    "NOTE: Maximum likelihood estimation cannot always be solved in an exact manner. The derivative of the log-likelihood function could be way too hard/impossible to differentiate.In such cases, iterative methods like Expectation-Maximization algorithms are used to find numerical solutions for the parameter estimates\n",
    "\n",
    "https://machinelearningmastery.com/expectation-maximization-em-algorithm/\n",
    "\n",
    "https://www.youtube.com/watch?v=93fPFOf547Q&list=RDCMUCjknLK_siVSCY14qfDu-f-w&start_radio=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065dafce",
   "metadata": {},
   "source": [
    "# Acknowledgements\n",
    "1. https://github.com/2796gaurav/Naive-bayes-explained\n",
    "2. https://chrisalbon.com/code/machine_learning/naive_bayes/naive_bayes_classifier_from_scratch/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef3dbf1",
   "metadata": {},
   "source": [
    "# More resources\n",
    "1. Implementation from total scratch (not even using Pandas) on multi class prediction for Iris Dataset https://machinelearningmastery.com/naive-bayes-classifier-scratch-python/ \n",
    "2. Other implementation https://towardsdatascience.com/implementing-naive-bayes-algorithm-from-scratch-python-c6880cfc9c41 (Once the concept is understood, custom Classifier classes can be written like here for maximum reuse of the technique) Github repo link also in article\n",
    "3. https://towardsdatascience.com/algorithms-from-scratch-naive-bayes-classifier-8006cc691493 Github repo link also in article\n",
    "4. https://ijcsmc.com/docs/papers/April2020/V9I4202015.pdf\n",
    "5. https://www.kaggle.com/vinayshaw/iris-species-100-accuracy-using-naive-bayes\n",
    "6. https://towardsdatascience.com/machine-learning-basics-naive-bayes-classification-964af6f2a965\n",
    "7. https://blog.floydhub.com/naive-bayes-for-machine-learning/\n",
    "8. https://medium.com/machine-learning-101/chapter-1-supervised-learning-and-naive-bayes-classification-part-1-theory-8b9e361897d5\n",
    "9. https://towardsdatascience.com/probability-concepts-explained-maximum-likelihood-estimation-c7b4342fdbb1\n",
    "10. Towards MAP as generalization of MLE https://towardsdatascience.com/probability-concepts-explained-bayesian-inference-for-parameter-estimation-90e8930e5348\n",
    "11. https://towardsdatascience.com/name-classification-with-naive-bayes-7c5e1415788a\n",
    "\n",
    "NOTE: Some of these resources are sometimes very light on implementing from grounds up and instead use Guassian Classifier from sklearn. Thats it! Use at your own risk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c12f8e8",
   "metadata": {},
   "source": [
    "# Books\n",
    "\n",
    "1. Think Bayes https://learning.oreilly.com/library/view/think-bayes-2nd/9781492089452/\n",
    "2. Bayesian Methods for Hackers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c656a810",
   "metadata": {},
   "source": [
    "# TODO\n",
    "1. Consider building a multi class predictor from scratch. Consider using OO approach\n",
    "2. Consider building a mesaure for accuracy of the Bayesian classifier prediction \n",
    "3. Using a confusion matrix to visualize the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039141fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
