{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4beee23",
   "metadata": {},
   "source": [
    "Gradient Descent is an optimization algorithm. Optimization is the process of maximizing or minimizing an objective function f(x) by searching for the appropriate variables x subject to some constraints cᵢ\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{x \\in R^{n}}\\ f(x)\\ subject\\ to\\  \\left\\{\\begin{array}{lll}\n",
    "                c_{i}(x)\\ = \\ 0,\\  i \\ \\in \\varepsilon, \\\\\n",
    "                c_{i}(x)\\ \\geq \\ 0,\\  i\\in \\iota \n",
    "            \\end{array}\\right.\n",
    "\\end{align*}\n",
    "\n",
    "where ℰ and ℐ are sets of indices for equality and inequality constraints, respectively.\n",
    "\n",
    "Two implementations of Gradient descent are presented\n",
    "1. Constant Learning rate - Easy\n",
    "2. Variable Learning rate - Hard - searches for the learning rate using Armijo line search. \n",
    "\n",
    "Gradient Descent algorithm can get trapped in a local minimum like many other optimization algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bac10f",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "Considering a simple objective function f(x) = x² − 2x − 3 the first derivative of f is, ∇f(x) = 2x − 2.\n",
    "\n",
    "We can use the exact analytical solution: take the derivative of f, then solve x such that the derivative is zero. We find that x minimizing f would satisfy ∇f(x) = 2x − 2 = 0, i.e., x = 1.\n",
    "Sometimes solving the derivating of f is hard or impossible. In such cases, we can solve the optimization problem by gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990417c7",
   "metadata": {},
   "source": [
    "# 2. Simple Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fc1cfd",
   "metadata": {},
   "source": [
    "# 3. Gradient Descent with varying Learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f17ccb0",
   "metadata": {},
   "source": [
    "# 4. Gradient Decent for Griewank Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706cac26",
   "metadata": {},
   "source": [
    "# 5. References\n",
    "Primary: https://towardsdatascience.com/complete-step-by-step-gradient-descent-algorithm-from-scratch-acba013e8420\n",
    "http://www.optimization-online.org/DB_FILE/2017/11/6350.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7ac567",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
